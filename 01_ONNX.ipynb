{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pthread/code/blob/main/01_ONNX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2b4a2ef",
      "metadata": {
        "id": "f2b4a2ef"
      },
      "source": [
        "## Inference under FHE for the MNIST Dataset using helayers\n",
        "\n",
        "In this demo we show how to run a Neural Network under encryption. The Neural Network is loaded as a tensor circuit: a circuit where each node is an tensor operator (matrix multiplication, sum-over-dim, convultion, etc...), and the inputs and outputs of each node are tensors.\n",
        "\n",
        "Under encryption we convert each tensor to a tile tensor: a data structure where the tensor is broken up into parts or tile, each tile stored encrypted in a ciphertext. Tile tensors support the same set of operators as tensors, and by configuring their shapes we can choose different packing options. This allows the system to easily convert the tensor circuit into a tile tensor circuit, and then an optimization step chooses the best shapes for them. Usually some auxiliary nodes need to be added as well: bootstrapping, duplication, cleanup of junk values from unused slots, etc.\n",
        "\n",
        "In this demo, we deal with a classification problem for the MNIST dataset [1], trying to correctly classify a batch of samples using a neural network model that will be created and trained using the Keras library (with architecture similar to reference [2]).\n",
        "\n",
        "We first build a plain neural network for the MNIST model that is based on Cryptonets [2]. Then, we'll encrypt the trained network and run inference over it using FHE. Larger model architetures are available in [HELayers](https://ibm.github.io/helayers/) samples. Specifically this sample is based on `09_Neural_network_MNIST` therein."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8611ceab",
      "metadata": {
        "id": "8611ceab"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Import timers\n",
        "from timeit import default_timer\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import Keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import utils, losses\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Activation\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Import the MNIST dataset\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import h5py\n",
        "\n",
        "train_batch_size = 500\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df81cc9",
      "metadata": {
        "id": "4df81cc9"
      },
      "source": [
        "#### Step 1. Preparing the data - loading and preprocessing the MNIST Dataset.\n",
        "\n",
        "In this tutorial we use the MNIST dataset that uses samples of dimension $28 \\times 28 \\times 1$, which we pad with zeros to a dimension of $29 \\times 29 \\times 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7245992",
      "metadata": {
        "id": "c7245992"
      },
      "outputs": [],
      "source": [
        "def process_data(l):\n",
        "    l = np.expand_dims(l.astype('float32'), -1)\n",
        "    l /= 255\n",
        "    return np.pad(l, ((0, 0), (0, 1), (0, 1), (0, 0)))\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = process_data(x_train)\n",
        "x_test = process_data(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ccea3dc",
      "metadata": {
        "id": "2ccea3dc"
      },
      "source": [
        "We generate a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77076339",
      "metadata": {
        "id": "77076339"
      },
      "outputs": [],
      "source": [
        "testSize=16\n",
        "x_val = x_test[:-testSize]\n",
        "x_test = x_test[-testSize:]\n",
        "y_val = y_test[:-testSize]\n",
        "y_test = y_test[-testSize:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a5f1d5c",
      "metadata": {
        "id": "2a5f1d5c"
      },
      "source": [
        "and we process the labels to have a one-hot representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f944e2b",
      "metadata": {
        "id": "2f944e2b"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "y_train = utils.to_categorical(y_train, num_classes)\n",
        "y_test = utils.to_categorical(y_test, num_classes)\n",
        "y_val = utils.to_categorical(y_val, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "224145a3",
      "metadata": {
        "id": "224145a3"
      },
      "source": [
        "Finally, we save the data as h5 files in the `output_dir` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30737363",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30737363",
        "outputId": "4d49f27b-ea83-437a-c3dd-3f2306a7b3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving x_test of shape (16, 29, 29, 1) in out/x_test.h5\n",
            "Saving x_train of shape (60000, 29, 29, 1) in out/x_train.h5\n",
            "Saving x_val of shape (9984, 29, 29, 1) in out/x_val.h5\n"
          ]
        }
      ],
      "source": [
        "output_dir = 'out'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "def save_data_set(x, y, data_type, s=''):\n",
        "    fname=os.path.join(output_dir, f'x_{data_type}{s}.h5')\n",
        "    print(\"Saving x_{} of shape {} in {}\".format(data_type, x.shape,fname))\n",
        "    xf = h5py.File(fname, 'w')\n",
        "    xf.create_dataset('x_{}'.format(data_type), data=x)\n",
        "    xf.close()\n",
        "\n",
        "    yf = h5py.File(os.path.join(output_dir, f'y_{data_type}{s}.h5'), 'w')\n",
        "    yf.create_dataset(f'y_{data_type}', data=y)\n",
        "    yf.close()\n",
        "\n",
        "save_data_set(x_test, y_test, data_type='test')\n",
        "save_data_set(x_train, y_train, data_type='train')\n",
        "save_data_set(x_val, y_val, data_type='val')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f36592d",
      "metadata": {
        "id": "3f36592d"
      },
      "source": [
        "### Step 2. Building the NN model\n",
        "\n",
        "For the demonstartion, we use a simple [cryptonets model](https://proceedings.mlr.press/v48/gilad-bachrach16.html) that was the first HE-friendly NN. I.e., a NN where all the operations are polynomials. The network includes one convolutional layer with a kernel of size $5 \\times 5$, and subsequently two fully connected (FC) layers separated by square activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "210b41ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "210b41ab",
        "outputId": "8bb75130-aa40-43e4-a374-06ed649af433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 13, 13, 5)         130       \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 845)               0         \n",
            "                                                                 \n",
            " square_activation_4 (Squar  (None, 845)               0         \n",
            " eActivation)                                                    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 100)               84600     \n",
            "                                                                 \n",
            " square_activation_5 (Squar  (None, 100)               0         \n",
            " eActivation)                                                    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85740 (334.92 KB)\n",
            "Trainable params: 85740 (334.92 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "class SquareActivation(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        return tf.math.square(inputs)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=5, kernel_size=5, strides=2, padding='valid', input_shape=x_train[0].shape))\n",
        "model.add(Flatten())\n",
        "model.add(SquareActivation())\n",
        "model.add(Dense(100))\n",
        "model.add(SquareActivation())\n",
        "model.add(Dense(num_classes))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be64456",
      "metadata": {
        "id": "8be64456"
      },
      "source": [
        "#### Training the network\n",
        "\n",
        "For training the network we use the Adam optimizer where we set the loss function to be the sum of the square errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f769d4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f769d4f",
        "outputId": "df2dde97-01b2-4f14-82d7-34b9eb3bf1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "120/120 - 8s - loss: 0.3830 - accuracy: 0.8482 - val_loss: 0.2028 - val_accuracy: 0.9508 - 8s/epoch - 65ms/step\n",
            "Epoch 2/10\n",
            "120/120 - 5s - loss: 0.1749 - accuracy: 0.9598 - val_loss: 0.1467 - val_accuracy: 0.9686 - 5s/epoch - 42ms/step\n",
            "Epoch 3/10\n",
            "120/120 - 5s - loss: 0.1366 - accuracy: 0.9706 - val_loss: 0.1237 - val_accuracy: 0.9727 - 5s/epoch - 41ms/step\n",
            "Epoch 4/10\n",
            "120/120 - 7s - loss: 0.1177 - accuracy: 0.9757 - val_loss: 0.1116 - val_accuracy: 0.9761 - 7s/epoch - 58ms/step\n",
            "Epoch 5/10\n",
            "120/120 - 5s - loss: 0.1058 - accuracy: 0.9786 - val_loss: 0.1037 - val_accuracy: 0.9774 - 5s/epoch - 40ms/step\n",
            "Epoch 6/10\n",
            "120/120 - 7s - loss: 0.0975 - accuracy: 0.9811 - val_loss: 0.0962 - val_accuracy: 0.9787 - 7s/epoch - 55ms/step\n",
            "Epoch 7/10\n",
            "120/120 - 5s - loss: 0.0914 - accuracy: 0.9826 - val_loss: 0.0918 - val_accuracy: 0.9808 - 5s/epoch - 44ms/step\n",
            "Epoch 8/10\n",
            "120/120 - 5s - loss: 0.0868 - accuracy: 0.9841 - val_loss: 0.0891 - val_accuracy: 0.9813 - 5s/epoch - 40ms/step\n",
            "Epoch 9/10\n",
            "120/120 - 7s - loss: 0.0831 - accuracy: 0.9853 - val_loss: 0.0860 - val_accuracy: 0.9826 - 7s/epoch - 57ms/step\n",
            "Epoch 10/10\n",
            "120/120 - 5s - loss: 0.0797 - accuracy: 0.9861 - val_loss: 0.0842 - val_accuracy: 0.9837 - 5s/epoch - 40ms/step\n",
            "Test loss: 0.037\n",
            "Test accuracy: 100.000%\n"
          ]
        }
      ],
      "source": [
        "def sum_squared_error(y_true, y_pred):\n",
        "    return K.sum(K.square(y_pred - y_true), axis=-1)\n",
        "\n",
        "model.compile(loss=sum_squared_error, optimizer='Adam', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=train_batch_size, epochs=epochs, verbose=2, validation_data=(x_val, y_val), shuffle=True)\n",
        "\n",
        "# Compute and print the test score\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test loss: { score[0]:.3f}')\n",
        "print(f'Test accuracy: {score[1] * 100:.3f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa85fbf3",
      "metadata": {
        "id": "aa85fbf3"
      },
      "source": [
        "#### Serializing the model (arch and weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197cdb6b",
      "metadata": {
        "id": "197cdb6b"
      },
      "outputs": [],
      "source": [
        "# Serialize the model architecture\n",
        "model_json = model.to_json()\n",
        "with open(os.path.join(output_dir, 'model.json'), \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Serialize the model weights to HDF5\n",
        "model.save_weights(os.path.join(output_dir, 'model.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afc4d8f0",
      "metadata": {
        "id": "afc4d8f0"
      },
      "source": [
        "# Step 3 Running the model using FHE\n",
        "\n",
        "Here we start by loading a plain NN model (nnp) load its content from the `json` and `h5` files.\n",
        "The `hyper_params` object can be used for various flags, e.g., we can tell the system to just load the architecture and use random weights. Here we keep it empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3642bff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3642bff",
        "outputId": "147b3112-b67d-43ce-c1b8-994d3888db2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyhelayers in /usr/local/lib/python3.10/dist-packages (1.5.3.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from pyhelayers) (1.23.5)\n",
            "Imported pyhelayers 1.5.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyhelayers\n",
        "import pyhelayers\n",
        "print('Imported pyhelayers',pyhelayers.VERSION)\n",
        "\n",
        "hyper_params = pyhelayers.PlainModelHyperParams()\n",
        "\n",
        "#initialize the NN architecture and weights from the json and h5 files that we stored before.\n",
        "nnp = pyhelayers.NeuralNetPlain()\n",
        "nnp.init_from_files(hyper_params, [os.path.join(output_dir, \"model.json\"), os.path.join(output_dir, \"model.h5\")])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca1b33be",
      "metadata": {
        "id": "ca1b33be"
      },
      "source": [
        "We are now ready to build an encrypted version of our NN model using tile tensors. However, as we saw defining the right shapes can be a hard task. Specifically, when the configuration includes multiple HE-related parameters, many of them depend on one another and on the specific NN model we defined.  \n",
        "\n",
        "To avoid this complexity, we will use the HE profile optimizer. This optimizer finds a configuration that is guaranteed to be secure and feasible. Furthermore, the optimizer receives the plain NN we have built and considers what's optimal for this very model in terms of performance.\n",
        "\n",
        "We can notify the optimizer of various requirements we have for running the model, with respect to the library and packaging considerations. For example here we ask to optimize for a batch size of 16 samples.\n",
        "\n",
        "The optimizer is called when compiling the model, given HE run requirements and the plain model. The result returned is a model `profile`, describing how the library and the encrypted NN should be initialized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dc14599",
      "metadata": {
        "id": "2dc14599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237d4a0d-bb2a-42ff-c651-c16fccd9435e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HE profileHe configuration requirement:\n",
            "Security level: 128\n",
            "Integer part precision: 10\n",
            "Fractional part precision: 50\n",
            "Number of slots: 8192\n",
            "Multiplication depth: 6\n",
            "Bootstrappable: False\n",
            "Automatic bootstrapping: False\n",
            "Rotation keys policy: custom, 15 keys required:\n",
            "[-512, -256, -128, -64, -32, -16, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
            "HE context name: SEAL_CKKS\n",
            "Mode: predict\n",
            "Tile layout: ( 8 x 64 x 16 )\n",
            "Mode name: conv_image_to_col\n",
            "Is model encrypted: true\n",
            "Using circuit optimization: false\n",
            "Lazy encoding: false\n",
            "Handle overflow: false\n",
            "Base chain index: 6\n",
            "Use AES inputs: false\n",
            "Use generically-packed inputs: false\n",
            "Optimal batch size: 16\n",
            "Model measuresRequired bootstrap operations: 0\n",
            "Estimated predict CPU time (s): 3.27\n",
            "Estimated init model CPU time (s): 4.15\n",
            "Estimated encrypt input CPU time (s): 1.02\n",
            "Estimated decrypt output CPU time (s): 0.00\n",
            "Estimated throughput (samples/s): 4.89\n",
            "Estimated model memory (MB): 384.07\n",
            "Estimated input memory (MB): 102.77\n",
            "Estimated output memory (MB): 0.26\n",
            "Estimated context memory (MB): 252.64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pred_batch_size=16\n",
        "\n",
        "he_run_req = pyhelayers.HeRunRequirements()\n",
        "he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
        "he_run_req.optimize_for_batch_size(pred_batch_size)\n",
        "\n",
        "# Set the requirements and run the model\n",
        "profile = pyhelayers.HeModel.compile(nnp, he_run_req)\n",
        "print(profile)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7187f9e2",
      "metadata": {
        "id": "7187f9e2"
      },
      "source": [
        "The HE profile includes a set of requirements from the library, such as the security level, precision, size of ciphertext, multiplication depth etc. Some of these can be set by the user and some others were found by the optimizer based on the NN architecture we defined. We'll now take this requirement object and use it to initialize the library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "036dcfe8",
      "metadata": {
        "id": "036dcfe8"
      },
      "outputs": [],
      "source": [
        "context = pyhelayers.HeModel.create_context(profile)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b909ef0",
      "metadata": {
        "id": "7b909ef0"
      },
      "source": [
        "We are ready to initialize the HE NN and populate its weights. Since we have already built a plain NN describing the model architecture and containing the weights, we'll simply encrypt into an encrypted version of the NN.\n",
        "\n",
        "Notice we provide the profile that was recommended by the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6af4fd6",
      "metadata": {
        "id": "d6af4fd6"
      },
      "outputs": [],
      "source": [
        "nn = pyhelayers.NeuralNet(context)\n",
        "nn.encode_encrypt(nnp, profile)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ebf1624",
      "metadata": {
        "id": "9ebf1624"
      },
      "source": [
        "### Step 4 running an inference operation\n",
        "We now load and classify real samples of the MNIST dataset from the stored HDF5 files, where we extract the first batch of samples and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91fd2a35",
      "metadata": {
        "id": "91fd2a35"
      },
      "outputs": [],
      "source": [
        "with h5py.File(os.path.join(output_dir, \"x_test.h5\")) as f:\n",
        "    x_test = np.array(f[\"x_test\"])\n",
        "with h5py.File(os.path.join(output_dir, \"y_test.h5\")) as f:\n",
        "    y_test = np.array(f[\"y_test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe22c23f",
      "metadata": {
        "id": "fe22c23f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c69cbb3-a3ee-4bfe-964f-4fa4569202ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of size 16 loaded\n"
          ]
        }
      ],
      "source": [
        "def extract_batch(x_test, y_test, batch_size, batch_num):\n",
        "    num_samples = x_test.shape[0]\n",
        "    num_lebels = y_test.shape[0]\n",
        "\n",
        "     # assert same size\n",
        "    assert(num_samples == num_lebels)\n",
        "\n",
        "    # calc start and end index\n",
        "    start_index = batch_num * batch_size\n",
        "    if start_index >= num_samples:\n",
        "        raise RuntimeError('Not enough samples for batch number ' +\n",
        "                           str(batch_num) + ' when batch size is ' + str(batch_size))\n",
        "    end_index = min(start_index + batch_size, num_samples)\n",
        "\n",
        "    batch_x = x_test.take(indices=range(start_index, end_index), axis=0)\n",
        "    batch_y = y_test.take(indices=range(start_index, end_index), axis=0)\n",
        "\n",
        "    return (batch_x, batch_y)\n",
        "\n",
        "plain_samples, labels = extract_batch(x_test, y_test, pred_batch_size, 0)\n",
        "print('Batch of size',pred_batch_size,'loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e465c00",
      "metadata": {
        "id": "1e465c00"
      },
      "source": [
        "To populate the input, we need to encode and then encrypt the values of the plain input under HE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3c5e214",
      "metadata": {
        "id": "a3c5e214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe51478-226e-464c-ff8a-c441cc2eb3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data encrypted\n"
          ]
        }
      ],
      "source": [
        "iop = nn.create_io_processor()\n",
        "samples = pyhelayers.EncryptedData(context)\n",
        "iop.encode_encrypt_inputs_for_predict(samples,[plain_samples])\n",
        "print('Test data encrypted')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e3e3652",
      "metadata": {
        "id": "4e3e3652"
      },
      "source": [
        "We now go ahead with the inference itself. We run the encrypted input through the encrypted NN to obtain encrypted results. This computation does not use the secret key and acts on completely encrypted values. Running the inference is done using the \"predict\" method of the NN, that receives the encrypted input tile tensor and produces as encrypted output tile tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "effd8f9f",
      "metadata": {
        "id": "effd8f9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d2e0af-2333-4527-8850-fdf53fb9aaeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latency = 7.89263 seconds, Amortized latency=0.49329 seconds\n"
          ]
        }
      ],
      "source": [
        "startTime = default_timer()\n",
        "\n",
        "predictions = pyhelayers.EncryptedData(context)\n",
        "nn.predict(predictions,samples)\n",
        "\n",
        "latency    = round(default_timer() - startTime,5)\n",
        "amortized_latency = round(latency/pred_batch_size,5)\n",
        "print(f\"Latency = {latency} seconds, Amortized latency={amortized_latency} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17362bbb",
      "metadata": {
        "id": "17362bbb"
      },
      "source": [
        "In order to assess the results of the computation, we first need to decrypt them. This is done by an IO processor that has the secret key and is capable of decrypting the ciphertext and decoding it into plaintext version of the HE computation result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e41bd86",
      "metadata": {
        "id": "2e41bd86"
      },
      "outputs": [],
      "source": [
        "plain_predictions = iop.decrypt_decode_output(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a61c1e2b",
      "metadata": {
        "tags": [],
        "id": "a61c1e2b"
      },
      "source": [
        "Let's verify the results are close to what we'd get from the plain network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "619d0605",
      "metadata": {
        "id": "619d0605",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3501749e-e5f5-4f1f-a7ae-bd2768c9a976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 distance between HE and plain predictions 2.417642015376116e-09\n"
          ]
        }
      ],
      "source": [
        "expected_pred=nnp.predict([plain_samples])\n",
        "diff=expected_pred-plain_predictions\n",
        "print('L2 distance between HE and plain predictions',np.linalg.norm(diff))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb1cbf65",
      "metadata": {
        "id": "fb1cbf65"
      },
      "source": [
        "Let's repeat the optimization and inference, but now for a different batch size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbb6c8f5",
      "metadata": {
        "id": "dbb6c8f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e306c98c-b4fb-4644-b860-84262e79ce5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HE profileHe configuration requirement:\n",
            "Security level: 128\n",
            "Integer part precision: 10\n",
            "Fractional part precision: 50\n",
            "Number of slots: 8192\n",
            "Multiplication depth: 6\n",
            "Bootstrappable: False\n",
            "Automatic bootstrapping: False\n",
            "Rotation keys policy: custom, 12 keys required:\n",
            "[-1024, -512, -256, -128, -64, 64, 128, 256, 512, 1024, 2048, 4096]\n",
            "HE context name: SEAL_CKKS\n",
            "Mode: predict\n",
            "Tile layout: ( 4 x 32 x 64 )\n",
            "Mode name: conv_image_to_col\n",
            "Is model encrypted: true\n",
            "Using circuit optimization: false\n",
            "Lazy encoding: false\n",
            "Handle overflow: false\n",
            "Base chain index: 6\n",
            "Use AES inputs: false\n",
            "Use generically-packed inputs: false\n",
            "Optimal batch size: 64\n",
            "Model measuresRequired bootstrap operations: 0\n",
            "Estimated predict CPU time (s): 6.26\n",
            "Estimated init model CPU time (s): 14.14\n",
            "Estimated encrypt input CPU time (s): 3.43\n",
            "Estimated decrypt output CPU time (s): 0.00\n",
            "Estimated throughput (samples/s): 10.22\n",
            "Estimated model memory (MB): 1313.72\n",
            "Estimated input memory (MB): 346.84\n",
            "Estimated output memory (MB): 0.26\n",
            "Estimated context memory (MB): 208.60\n",
            "\n",
            "Batch of size 64 loaded\n",
            "Latency = 10.73434 seconds, Amortized latency=0.16772 seconds\n"
          ]
        }
      ],
      "source": [
        "pred_batch_size=64\n",
        "\n",
        "\n",
        "he_run_req = pyhelayers.HeRunRequirements()\n",
        "he_run_req.set_he_context_options([pyhelayers.DefaultContext()])\n",
        "he_run_req.optimize_for_batch_size(pred_batch_size)\n",
        "\n",
        "# Set the requirements and run the model\n",
        "profile = pyhelayers.HeModel.compile(nnp, he_run_req)\n",
        "print(profile)\n",
        "\n",
        "context = pyhelayers.HeModel.create_context(profile)\n",
        "\n",
        "nn = pyhelayers.NeuralNet(context)\n",
        "nn.encode_encrypt(nnp, profile)\n",
        "\n",
        "plain_samples, labels = extract_batch(x_test, y_test, pred_batch_size, 0)\n",
        "print('Batch of size',pred_batch_size,'loaded')\n",
        "\n",
        "iop = nn.create_io_processor()\n",
        "samples = pyhelayers.EncryptedData(context)\n",
        "iop.encode_encrypt_inputs_for_predict(samples,[plain_samples])\n",
        "\n",
        "startTime = default_timer()\n",
        "\n",
        "predictions = pyhelayers.EncryptedData(context)\n",
        "nn.predict(predictions,samples)\n",
        "\n",
        "latency    = round(default_timer() - startTime,5)\n",
        "amortized_latency = round(latency/pred_batch_size,5)\n",
        "print(f\"Latency = {latency} seconds, Amortized latency={amortized_latency} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a015bf9d",
      "metadata": {
        "id": "a015bf9d"
      },
      "source": [
        "<br>\n",
        "\n",
        "References:\n",
        "\n",
        "<sub><sup> 1.\tLeCun, Yann and Cortes, Corinna. \"MNIST handwritten digit database.\" (2010): </sup></sub>\n",
        "\n",
        "<sub><sup> 2.\tGilad-Bachrach, R., Dowlin, N., Laine, K., Lauter, K., Naehrig, M. &amp; Wernsing, J.. (2016). CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy. Proceedings of The 33rd International Conference on Machine Learning, in Proceedings of Machine Learning Research 48:201-210 Available from https://proceedings.mlr.press/v48/gilad-bachrach16.html.\n",
        "</sup></sub>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}